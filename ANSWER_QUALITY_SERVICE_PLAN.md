# Answer Quality & Feedback Service - Implementation Plan

## Executive Summary

This plan outlines the implementation of a new **Answer Quality & Feedback Service** for FactorialBot's RAG (Retrieval Augmented Generation) chatbot. Unlike traditional sentiment analysis, this service focuses on measuring what matters most for knowledge-base chatbots: **Did the user get their answer?**

### Why Answer Quality Over Sentiment?

For RAG chatbots where users query documents/websites:
- 90% of conversations are neutral informational queries
- Negative sentiment usually indicates **bad answers**, not emotional distress
- Direct feedback (üëç/üëé) provides clearer signals than sentiment inference
- RAG quality metrics identify systemic issues (missing docs, poor retrieval)
- Knowledge gap detection guides content improvement

### Key Benefits

1. **Measurable ROI**: Track answer helpfulness percentage, resolution rates
2. **Actionable Insights**: Identify missing documents, improve retrieval
3. **User-Centric**: Simple thumbs up/down feedback (no ML required)
4. **Admin Dashboard**: See quality metrics, knowledge gaps, improvement areas
5. **Faster Implementation**: 2-3 weeks vs 3-4 weeks for sentiment analysis

### Architecture Overview

- **Microservice**: Separate FastAPI service (port 8005)
- **Event-Driven**: RabbitMQ consumers for async processing
- **Reference Pattern**: No message duplication (stores message_id + metrics only)
- **Optional Sentiment**: Basic VADER for frustration detection (no ML infrastructure)

---

## 1. Service Architecture

### 1.1 Directory Structure

```
answer-quality-service/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feedback.py              # AnswerFeedback model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quality_metrics.py      # RAGQualityMetrics model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ knowledge_gap.py        # KnowledgeGap model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_quality.py      # SessionQuality model
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feedback.py              # User feedback endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ admin_quality.py         # Admin dashboard endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.py                # Health check
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feedback_service.py      # Feedback collection logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quality_analyzer.py      # RAG quality analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gap_detector.py          # Knowledge gap detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentiment_basic.py       # Optional VADER sentiment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rabbitmq_consumer.py     # Event consumer
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feedback.py              # Feedback DTOs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quality.py               # Quality metric DTOs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py             # Dashboard DTOs
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                # Configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py              # Database connection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging_config.py        # Structured logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth.py                  # JWT validation
‚îÇ   ‚îî‚îÄ‚îÄ middleware/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ tenant_context.py        # Tenant isolation
‚îú‚îÄ‚îÄ alembic/
‚îÇ   ‚îú‚îÄ‚îÄ versions/
‚îÇ   ‚îî‚îÄ‚îÄ env.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_feedback.py
‚îÇ   ‚îú‚îÄ‚îÄ test_quality_analyzer.py
‚îÇ   ‚îú‚îÄ‚îÄ test_gap_detector.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ alembic.ini
‚îî‚îÄ‚îÄ README.md
```

### 1.2 Database Schema

```sql
-- Answer feedback from users
CREATE TABLE answer_feedback (
    id VARCHAR(36) PRIMARY KEY,
    tenant_id VARCHAR(36) NOT NULL,
    session_id VARCHAR(36) NOT NULL,
    message_id VARCHAR(36) NOT NULL,  -- References chat_messages.id
    feedback_type VARCHAR(20) NOT NULL,  -- 'helpful', 'not_helpful'
    feedback_comment TEXT,  -- Optional user comment
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    CONSTRAINT fk_tenant FOREIGN KEY (tenant_id) REFERENCES tenants(id),
    INDEX idx_tenant_feedback (tenant_id, created_at),
    INDEX idx_message_feedback (message_id),
    INDEX idx_session_feedback (session_id)
);

-- RAG quality metrics per message
CREATE TABLE rag_quality_metrics (
    id VARCHAR(36) PRIMARY KEY,
    tenant_id VARCHAR(36) NOT NULL,
    session_id VARCHAR(36) NOT NULL,
    message_id VARCHAR(36) NOT NULL,  -- References chat_messages.id

    -- RAG Metrics
    retrieval_score FLOAT,  -- Average relevance of retrieved docs (0-1)
    documents_retrieved INTEGER,  -- Number of docs used
    answer_confidence FLOAT,  -- LLM confidence score (0-1)
    sources_cited INTEGER,  -- Number of sources cited in response

    -- Answer Characteristics
    answer_length INTEGER,  -- Character count of AI response
    response_time_ms INTEGER,  -- Generation time in milliseconds

    -- Optional Basic Sentiment
    basic_sentiment VARCHAR(20),  -- 'positive', 'neutral', 'negative', 'frustrated'
    sentiment_confidence FLOAT,  -- VADER compound score

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    CONSTRAINT fk_tenant FOREIGN KEY (tenant_id) REFERENCES tenants(id),
    INDEX idx_tenant_quality (tenant_id, created_at),
    INDEX idx_message_quality (message_id),
    INDEX idx_low_confidence (tenant_id, answer_confidence)  -- Find low-quality answers
);

-- Knowledge gaps detected
CREATE TABLE knowledge_gaps (
    id VARCHAR(36) PRIMARY KEY,
    tenant_id VARCHAR(36) NOT NULL,

    -- Gap Information
    question_pattern TEXT NOT NULL,  -- Generalized question pattern
    example_questions JSON,  -- Array of actual user questions
    occurrence_count INTEGER DEFAULT 1,  -- How many times asked

    -- Quality Indicators
    avg_confidence FLOAT,  -- Average confidence of answers
    negative_feedback_count INTEGER DEFAULT 0,  -- Thumbs down count

    -- Gap Status
    status VARCHAR(20) DEFAULT 'detected',  -- 'detected', 'acknowledged', 'resolved'
    resolved_at TIMESTAMP WITH TIME ZONE,
    resolution_notes TEXT,

    first_detected_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_occurrence_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    CONSTRAINT fk_tenant FOREIGN KEY (tenant_id) REFERENCES tenants(id),
    INDEX idx_tenant_gaps (tenant_id, status),
    INDEX idx_gap_severity (tenant_id, occurrence_count, avg_confidence)
);

-- Session-level quality summary
CREATE TABLE session_quality (
    id VARCHAR(36) PRIMARY KEY,
    tenant_id VARCHAR(36) NOT NULL,
    session_id VARCHAR(36) NOT NULL,  -- References chat_sessions.id

    -- Overall Metrics
    total_messages INTEGER DEFAULT 0,
    messages_with_feedback INTEGER DEFAULT 0,
    helpful_count INTEGER DEFAULT 0,
    not_helpful_count INTEGER DEFAULT 0,

    -- Quality Scores
    avg_retrieval_score FLOAT,
    avg_confidence_score FLOAT,
    avg_response_time_ms INTEGER,

    -- Session Outcome
    session_success BOOLEAN,  -- Did user achieve their goal?
    success_indicators JSON,  -- {"resolved": true, "feedback": "positive"}

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    CONSTRAINT fk_tenant FOREIGN KEY (tenant_id) REFERENCES tenants(id),
    UNIQUE INDEX idx_session_unique (session_id),
    INDEX idx_tenant_sessions (tenant_id, created_at)
);
```

### 1.3 Dependencies (requirements.txt)

```txt
# FastAPI framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0

# Database
sqlalchemy==2.0.23
alembic==1.12.1
psycopg2-binary==2.9.9

# Authentication
python-jose[cryptography]==3.3.0
python-multipart==0.0.6

# RabbitMQ
pika==1.3.2

# Redis (for caching)
redis==5.0.1

# HTTP client (for chat-service API)
httpx==0.25.2

# Logging
structlog==23.2.0
loguru==0.7.2

# Sentiment Analysis (Optional - VADER)
vaderSentiment==3.3.2

# Utilities
python-dateutil==2.8.2

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
httpx==0.25.2  # For API testing

# Code quality
black==23.11.0
flake8==6.1.0
mypy==1.7.1
```

### 1.4 Environment Configuration (.env.example)

```bash
# Service Configuration
ENVIRONMENT=development
LOG_LEVEL=INFO
SERVICE_NAME=answer-quality-service
API_V1_STR=/api/v1

# Server
HOST=0.0.0.0
PORT=8005

# Database
DATABASE_URL=postgresql://postgres:password@localhost:5432/answer_quality_db

# Redis
REDIS_URL=redis://localhost:6379/4

# RabbitMQ
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5672
RABBITMQ_USER=guest
RABBITMQ_PASSWORD=guest
RABBITMQ_VHOST=/

# RabbitMQ Queues
QUEUE_CHAT_MESSAGES=answer_quality.chat_messages
QUEUE_FEEDBACK_EVENTS=answer_quality.feedback_events

# RabbitMQ Exchanges
EXCHANGE_CHAT_EVENTS=chat.events
EXCHANGE_QUALITY_EVENTS=quality.events

# Chat Service Integration
CHAT_SERVICE_URL=http://localhost:8000
CHAT_SERVICE_TIMEOUT=10

# Authorization Server
AUTH_SERVER_URL=http://localhost:9000

# JWT Configuration
# Note: JWT_SECRET_KEY must be in environment, not here
JWT_ALGORITHM=RS256
JWT_ISSUER=http://localhost:9000

# Feature Flags
ENABLE_BASIC_SENTIMENT=true
ENABLE_GAP_DETECTION=true
ENABLE_SESSION_TRACKING=true

# Quality Thresholds
LOW_CONFIDENCE_THRESHOLD=0.5
GAP_DETECTION_THRESHOLD=3  # Detect gap after 3 similar low-confidence questions
SESSION_TIMEOUT_MINUTES=30

# Admin Dashboard
ENABLE_ADMIN_DASHBOARD=true
```

---

## 2. Core Features Implementation

### 2.1 User Feedback Collection

**Goal**: Allow users to provide thumbs up/down feedback on AI responses

**API Endpoints**:

```python
# app/api/feedback.py

from fastapi import APIRouter, Depends, HTTPException, status
from app.schemas.feedback import FeedbackCreate, FeedbackResponse
from app.services.feedback_service import FeedbackService
from app.core.auth import validate_token

router = APIRouter(prefix="/api/v1/feedback", tags=["feedback"])

@router.post("/", response_model=FeedbackResponse, status_code=status.HTTP_201_CREATED)
async def submit_feedback(
    feedback: FeedbackCreate,
    claims: dict = Depends(validate_token),
    service: FeedbackService = Depends()
):
    """
    Submit feedback for an AI response.

    User can provide:
    - feedback_type: 'helpful' or 'not_helpful'
    - Optional comment explaining the feedback
    """
    tenant_id = claims.get("tenant_id")

    result = await service.submit_feedback(
        tenant_id=tenant_id,
        message_id=feedback.message_id,
        session_id=feedback.session_id,
        feedback_type=feedback.feedback_type,
        comment=feedback.comment
    )

    return result

@router.get("/message/{message_id}", response_model=FeedbackResponse)
async def get_message_feedback(
    message_id: str,
    claims: dict = Depends(validate_token),
    service: FeedbackService = Depends()
):
    """Get feedback for a specific message"""
    tenant_id = claims.get("tenant_id")
    return await service.get_message_feedback(tenant_id, message_id)
```

**Service Implementation**:

```python
# app/services/feedback_service.py

from sqlalchemy.orm import Session
from app.models.feedback import AnswerFeedback
from app.services.gap_detector import GapDetector
import uuid

class FeedbackService:
    def __init__(self, db: Session, gap_detector: GapDetector):
        self.db = db
        self.gap_detector = gap_detector

    async def submit_feedback(
        self,
        tenant_id: str,
        message_id: str,
        session_id: str,
        feedback_type: str,
        comment: str = None
    ):
        """
        Submit user feedback and trigger downstream processing
        """
        # Create feedback record
        feedback = AnswerFeedback(
            id=str(uuid.uuid4()),
            tenant_id=tenant_id,
            session_id=session_id,
            message_id=message_id,
            feedback_type=feedback_type,
            feedback_comment=comment
        )

        self.db.add(feedback)
        self.db.commit()
        self.db.refresh(feedback)

        # If negative feedback, check for knowledge gaps
        if feedback_type == "not_helpful":
            await self.gap_detector.analyze_negative_feedback(
                tenant_id, message_id, session_id
            )

        # Update session quality metrics
        await self._update_session_quality(tenant_id, session_id)

        # Publish feedback event
        await self._publish_feedback_event(feedback)

        return feedback

    async def _update_session_quality(self, tenant_id: str, session_id: str):
        """Update session-level quality metrics"""
        # Count feedback for this session
        feedback_counts = self.db.query(
            func.count(AnswerFeedback.id).label('total'),
            func.sum(case((AnswerFeedback.feedback_type == 'helpful', 1), else_=0)).label('helpful'),
            func.sum(case((AnswerFeedback.feedback_type == 'not_helpful', 1), else_=0)).label('not_helpful')
        ).filter(
            AnswerFeedback.tenant_id == tenant_id,
            AnswerFeedback.session_id == session_id
        ).first()

        # Update or create session quality record
        # ... (implementation details)
```

**RabbitMQ Event Publishing**:

```python
# Publish to quality.events exchange
{
    "event_type": "feedback.submitted",
    "tenant_id": "uuid",
    "session_id": "uuid",
    "message_id": "uuid",
    "feedback_type": "not_helpful",
    "has_comment": true,
    "timestamp": "2025-01-17T10:30:00Z"
}
```

### 2.2 RAG Quality Metrics Tracking

**Goal**: Capture quality indicators from RAG pipeline (retrieval scores, confidence, sources)

**Implementation Strategy**:

The chat-service will be modified to publish quality metrics along with message creation events:

```python
# Modified chat-service event publishing

# When AI generates response, publish enhanced event
{
    "event_type": "message.created",
    "tenant_id": "uuid",
    "session_id": "uuid",
    "message_id": "uuid",
    "message_type": "assistant",
    "content_preview": "First 200 chars...",  # For reference

    # RAG Quality Metrics (NEW)
    "quality_metrics": {
        "retrieval_score": 0.85,  # Average relevance of retrieved docs
        "documents_retrieved": 5,
        "answer_confidence": 0.78,  # LLM confidence score
        "sources_cited": 3,
        "answer_length": 450,
        "response_time_ms": 1250
    },

    "timestamp": "2025-01-17T10:30:00Z"
}
```

**Quality Analyzer Service**:

```python
# app/services/quality_analyzer.py

class QualityAnalyzer:
    """Analyze RAG quality metrics and detect issues"""

    LOW_CONFIDENCE_THRESHOLD = 0.5
    LOW_RETRIEVAL_THRESHOLD = 0.4

    async def analyze_message_quality(
        self,
        tenant_id: str,
        message_id: str,
        session_id: str,
        metrics: dict
    ):
        """
        Store quality metrics and flag potential issues
        """
        quality_record = RAGQualityMetrics(
            id=str(uuid.uuid4()),
            tenant_id=tenant_id,
            session_id=session_id,
            message_id=message_id,
            retrieval_score=metrics.get("retrieval_score"),
            documents_retrieved=metrics.get("documents_retrieved"),
            answer_confidence=metrics.get("answer_confidence"),
            sources_cited=metrics.get("sources_cited"),
            answer_length=metrics.get("answer_length"),
            response_time_ms=metrics.get("response_time_ms")
        )

        # Optional: Add basic sentiment analysis
        if ENABLE_BASIC_SENTIMENT:
            sentiment = self._analyze_basic_sentiment(message_content)
            quality_record.basic_sentiment = sentiment["label"]
            quality_record.sentiment_confidence = sentiment["score"]

        self.db.add(quality_record)
        self.db.commit()

        # Check for quality issues
        if quality_record.answer_confidence < self.LOW_CONFIDENCE_THRESHOLD:
            await self._flag_low_confidence_answer(tenant_id, message_id, quality_record)

        if quality_record.retrieval_score < self.LOW_RETRIEVAL_THRESHOLD:
            await self._flag_poor_retrieval(tenant_id, message_id, quality_record)

        return quality_record

    def _analyze_basic_sentiment(self, text: str) -> dict:
        """
        Optional: Basic sentiment using VADER (rule-based, fast)
        Only used to detect frustration as proxy for bad answers
        """
        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

        analyzer = SentimentIntensityAnalyzer()
        scores = analyzer.polarity_scores(text)

        compound = scores['compound']

        if compound >= 0.05:
            label = "positive"
        elif compound <= -0.05:
            if compound <= -0.5:  # Strong negative = frustration
                label = "frustrated"
            else:
                label = "negative"
        else:
            label = "neutral"

        return {"label": label, "score": compound}
```

### 2.3 Knowledge Gap Detection

**Goal**: Identify topics/questions where the knowledge base is insufficient

**Gap Detection Algorithm**:

```python
# app/services/gap_detector.py

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class GapDetector:
    """Detect knowledge gaps from low-quality answers and negative feedback"""

    GAP_SIMILARITY_THRESHOLD = 0.7  # Questions are "similar" if > 0.7
    GAP_OCCURRENCE_THRESHOLD = 3    # Detect gap after 3 similar questions

    def __init__(self, db: Session):
        self.db = db
        self.vectorizer = TfidfVectorizer(stop_words='english')

    async def analyze_negative_feedback(
        self,
        tenant_id: str,
        message_id: str,
        session_id: str
    ):
        """
        When user gives negative feedback, check if this reveals a knowledge gap
        """
        # Get the user's question (previous message in conversation)
        user_question = await self._get_user_question(message_id)

        if not user_question:
            return

        # Get quality metrics for the AI response
        quality = self.db.query(RAGQualityMetrics).filter(
            RAGQualityMetrics.message_id == message_id
        ).first()

        # Only consider as potential gap if confidence was low
        if quality and quality.answer_confidence < 0.5:
            await self._check_for_gap(tenant_id, user_question, quality.answer_confidence)

    async def detect_low_confidence_patterns(self, tenant_id: str):
        """
        Batch job: Analyze all low-confidence answers to find patterns
        Runs daily via scheduler
        """
        # Get recent low-confidence messages
        low_confidence_messages = self.db.query(
            RAGQualityMetrics
        ).filter(
            RAGQualityMetrics.tenant_id == tenant_id,
            RAGQualityMetrics.answer_confidence < 0.5,
            RAGQualityMetrics.created_at >= datetime.now() - timedelta(days=7)
        ).all()

        if len(low_confidence_messages) < 3:
            return []  # Need at least 3 instances

        # Get corresponding user questions
        questions = []
        for metric in low_confidence_messages:
            question = await self._get_user_question(metric.message_id)
            if question:
                questions.append({
                    "text": question,
                    "message_id": metric.message_id,
                    "confidence": metric.answer_confidence
                })

        # Cluster similar questions using TF-IDF + cosine similarity
        question_texts = [q["text"] for q in questions]

        if len(question_texts) < 2:
            return []

        tfidf_matrix = self.vectorizer.fit_transform(question_texts)
        similarity_matrix = cosine_similarity(tfidf_matrix)

        # Find clusters of similar questions
        clusters = self._find_clusters(similarity_matrix, questions)

        # Create knowledge gap records for significant clusters
        gaps_detected = []
        for cluster in clusters:
            if len(cluster["questions"]) >= self.GAP_OCCURRENCE_THRESHOLD:
                gap = await self._create_knowledge_gap(tenant_id, cluster)
                gaps_detected.append(gap)

        return gaps_detected

    def _find_clusters(self, similarity_matrix, questions):
        """Group similar questions into clusters"""
        n = len(questions)
        visited = [False] * n
        clusters = []

        for i in range(n):
            if visited[i]:
                continue

            # Start new cluster
            cluster = {
                "questions": [questions[i]],
                "avg_confidence": questions[i]["confidence"]
            }
            visited[i] = True

            # Find similar questions
            for j in range(i + 1, n):
                if not visited[j] and similarity_matrix[i][j] >= self.GAP_SIMILARITY_THRESHOLD:
                    cluster["questions"].append(questions[j])
                    visited[j] = True

            # Calculate average confidence
            cluster["avg_confidence"] = np.mean([q["confidence"] for q in cluster["questions"]])

            clusters.append(cluster)

        return clusters

    async def _create_knowledge_gap(self, tenant_id: str, cluster: dict):
        """Create or update knowledge gap record"""

        # Generate question pattern (most common terms)
        question_texts = [q["text"] for q in cluster["questions"]]
        pattern = self._extract_pattern(question_texts)

        # Check if similar gap already exists
        existing_gap = self.db.query(KnowledgeGap).filter(
            KnowledgeGap.tenant_id == tenant_id,
            KnowledgeGap.question_pattern.ilike(f"%{pattern}%"),
            KnowledgeGap.status == "detected"
        ).first()

        if existing_gap:
            # Update existing gap
            existing_gap.occurrence_count += len(cluster["questions"])
            existing_gap.example_questions = (
                existing_gap.example_questions + question_texts
            )[:10]  # Keep max 10 examples
            existing_gap.avg_confidence = (
                (existing_gap.avg_confidence + cluster["avg_confidence"]) / 2
            )
            existing_gap.last_occurrence_at = datetime.now()
            self.db.commit()
            return existing_gap
        else:
            # Create new gap
            gap = KnowledgeGap(
                id=str(uuid.uuid4()),
                tenant_id=tenant_id,
                question_pattern=pattern,
                example_questions=question_texts[:5],  # Store first 5 examples
                occurrence_count=len(cluster["questions"]),
                avg_confidence=cluster["avg_confidence"],
                status="detected"
            )
            self.db.add(gap)
            self.db.commit()

            # Publish event
            await self._publish_gap_detected_event(gap)

            return gap

    def _extract_pattern(self, questions: list) -> str:
        """Extract common pattern from similar questions"""
        # Use TF-IDF to find most important terms across questions
        tfidf = self.vectorizer.fit_transform(questions)
        feature_names = self.vectorizer.get_feature_names_out()

        # Sum TF-IDF scores for each term
        term_scores = np.array(tfidf.sum(axis=0)).flatten()

        # Get top 5 terms
        top_indices = term_scores.argsort()[-5:][::-1]
        top_terms = [feature_names[i] for i in top_indices]

        return " ".join(top_terms)
```

**Knowledge Gap Events**:

```python
# Published to quality.events exchange
{
    "event_type": "knowledge.gap.detected",
    "tenant_id": "uuid",
    "gap_id": "uuid",
    "pattern": "pricing tier upgrade billing",
    "occurrence_count": 5,
    "avg_confidence": 0.32,
    "example_questions": [
        "How do I upgrade my pricing tier?",
        "What happens to my billing when I upgrade?",
        "Can I upgrade mid-billing cycle?"
    ],
    "timestamp": "2025-01-17T10:30:00Z"
}
```

### 2.4 Session Success Tracking

**Goal**: Determine if a chat session was successful (user got their answer)

**Success Indicators**:

```python
# app/services/session_tracker.py

class SessionTracker:
    """Track session-level quality and success"""

    def calculate_session_success(self, session_id: str, tenant_id: str) -> dict:
        """
        Determine if session was successful based on multiple signals
        """
        # Get all messages in session
        messages = self._get_session_messages(session_id)

        # Get feedback
        feedback = self._get_session_feedback(session_id)

        # Get quality metrics
        quality_metrics = self._get_session_quality_metrics(session_id)

        # Calculate success indicators
        indicators = {
            "has_positive_feedback": any(f.feedback_type == "helpful" for f in feedback),
            "no_negative_feedback": not any(f.feedback_type == "not_helpful" for f in feedback),
            "high_avg_confidence": self._calculate_avg_confidence(quality_metrics) > 0.7,
            "short_session": len(messages) <= 5,  # User didn't need to ask many times
            "no_repeat_questions": not self._has_repeat_questions(messages),
            "session_completed": self._session_ended_naturally(session_id)
        }

        # Success if majority of indicators are positive
        success_score = sum(indicators.values()) / len(indicators)
        session_success = success_score >= 0.6  # 60% threshold

        return {
            "success": session_success,
            "score": success_score,
            "indicators": indicators
        }
```

---

## 3. RabbitMQ Integration

### 3.1 Message Consumer Setup

```python
# app/services/rabbitmq_consumer.py

import pika
import json
from app.services.quality_analyzer import QualityAnalyzer
from app.services.feedback_service import FeedbackService

class RabbitMQConsumer:
    """Consume events from chat-service and other services"""

    def __init__(self):
        self.connection = None
        self.channel = None
        self.quality_analyzer = QualityAnalyzer()

    def connect(self):
        """Establish RabbitMQ connection"""
        credentials = pika.PlainCredentials(
            os.getenv("RABBITMQ_USER"),
            os.getenv("RABBITMQ_PASSWORD")
        )
        parameters = pika.ConnectionParameters(
            host=os.getenv("RABBITMQ_HOST"),
            port=int(os.getenv("RABBITMQ_PORT")),
            credentials=credentials,
            heartbeat=600,
            blocked_connection_timeout=300
        )

        self.connection = pika.BlockingConnection(parameters)
        self.channel = self.connection.channel()

        # Declare exchange
        self.channel.exchange_declare(
            exchange="chat.events",
            exchange_type="topic",
            durable=True
        )

        # Declare queue
        self.channel.queue_declare(
            queue="answer_quality.chat_messages",
            durable=True
        )

        # Bind queue to exchange
        self.channel.queue_bind(
            exchange="chat.events",
            queue="answer_quality.chat_messages",
            routing_key="message.created"
        )

    def start_consuming(self):
        """Start consuming messages"""
        self.channel.basic_qos(prefetch_count=1)
        self.channel.basic_consume(
            queue="answer_quality.chat_messages",
            on_message_callback=self.on_message
        )

        logger.info("Starting RabbitMQ consumer for answer quality service")
        self.channel.start_consuming()

    def on_message(self, channel, method, properties, body):
        """Handle incoming message"""
        try:
            event = json.loads(body)

            if event["event_type"] == "message.created":
                # Only process assistant messages (AI responses)
                if event.get("message_type") == "assistant":
                    await self.quality_analyzer.analyze_message_quality(
                        tenant_id=event["tenant_id"],
                        message_id=event["message_id"],
                        session_id=event["session_id"],
                        metrics=event.get("quality_metrics", {})
                    )

            # Acknowledge message
            channel.basic_ack(delivery_tag=method.delivery_tag)

        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            # Reject and requeue message
            channel.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
```

### 3.2 Event Publishing

```python
# app/services/event_publisher.py

class EventPublisher:
    """Publish quality-related events to RabbitMQ"""

    def __init__(self):
        self.connection = None
        self.channel = None

    def connect(self):
        """Establish RabbitMQ connection"""
        # Similar to consumer setup

        # Declare quality events exchange
        self.channel.exchange_declare(
            exchange="quality.events",
            exchange_type="topic",
            durable=True
        )

    def publish_feedback_event(self, feedback: AnswerFeedback):
        """Publish feedback submitted event"""
        event = {
            "event_type": "feedback.submitted",
            "tenant_id": feedback.tenant_id,
            "session_id": feedback.session_id,
            "message_id": feedback.message_id,
            "feedback_type": feedback.feedback_type,
            "has_comment": feedback.feedback_comment is not None,
            "timestamp": datetime.now().isoformat()
        }

        self.channel.basic_publish(
            exchange="quality.events",
            routing_key="feedback.submitted",
            body=json.dumps(event),
            properties=pika.BasicProperties(
                delivery_mode=2,  # Persistent
                content_type="application/json"
            )
        )

    def publish_gap_detected_event(self, gap: KnowledgeGap):
        """Publish knowledge gap detected event"""
        event = {
            "event_type": "knowledge.gap.detected",
            "tenant_id": gap.tenant_id,
            "gap_id": gap.id,
            "pattern": gap.question_pattern,
            "occurrence_count": gap.occurrence_count,
            "avg_confidence": gap.avg_confidence,
            "example_questions": gap.example_questions[:3],
            "timestamp": datetime.now().isoformat()
        }

        self.channel.basic_publish(
            exchange="quality.events",
            routing_key="knowledge.gap.detected",
            body=json.dumps(event),
            properties=pika.BasicProperties(
                delivery_mode=2,
                content_type="application/json"
            )
        )
```

---

## 4. Admin Dashboard Endpoints

### 4.1 Quality Overview

```python
# app/api/admin_quality.py

from fastapi import APIRouter, Depends, Query
from app.core.auth import validate_token, require_admin
from app.services.dashboard_service import DashboardService

router = APIRouter(prefix="/api/v1/admin/quality", tags=["admin-quality"])

@router.get("/dashboard")
async def get_quality_dashboard(
    days: int = Query(7, ge=1, le=90),
    claims: dict = Depends(validate_token),
    _: None = Depends(require_admin),
    service: DashboardService = Depends()
):
    """
    Get overall quality dashboard metrics

    Returns:
    - Total sessions in period
    - Success rate
    - Feedback statistics
    - Average quality scores
    - Knowledge gaps count
    """
    tenant_id = claims.get("tenant_id")

    return await service.get_dashboard_overview(tenant_id, days)

# Response example:
{
    "period_days": 7,
    "total_sessions": 245,
    "successful_sessions": 198,
    "success_rate": 0.81,
    "feedback": {
        "total_feedback": 156,
        "helpful": 128,
        "not_helpful": 28,
        "helpful_percentage": 0.82
    },
    "quality_scores": {
        "avg_retrieval_score": 0.74,
        "avg_confidence_score": 0.68,
        "avg_response_time_ms": 1450
    },
    "knowledge_gaps": {
        "total_gaps": 5,
        "new_gaps": 2,
        "resolved_gaps": 1
    },
    "sentiment_breakdown": {
        "positive": 45,
        "neutral": 180,
        "negative": 15,
        "frustrated": 5
    }
}
```

### 4.2 Sessions with Issues

```python
@router.get("/sessions/issues")
async def get_problematic_sessions(
    limit: int = Query(50, le=200),
    issue_type: str = Query(None, regex="^(low_confidence|negative_feedback|low_success)$"),
    claims: dict = Depends(validate_token),
    _: None = Depends(require_admin),
    service: DashboardService = Depends()
):
    """
    Get sessions that need admin attention

    Filters:
    - low_confidence: Sessions with avg confidence < 0.5
    - negative_feedback: Sessions with negative feedback
    - low_success: Sessions with low success indicators
    """
    tenant_id = claims.get("tenant_id")

    return await service.get_problematic_sessions(
        tenant_id, limit, issue_type
    )

# Response example:
{
    "sessions": [
        {
            "session_id": "uuid",
            "created_at": "2025-01-17T10:00:00Z",
            "total_messages": 8,
            "success_score": 0.33,
            "avg_confidence": 0.42,
            "negative_feedback_count": 2,
            "issues": ["low_confidence", "negative_feedback"],
            "preview": "User asked about pricing tiers..."
        }
    ],
    "total_count": 12
}
```

### 4.3 Knowledge Gaps

```python
@router.get("/gaps")
async def get_knowledge_gaps(
    status: str = Query("detected", regex="^(detected|acknowledged|resolved)$"),
    min_occurrences: int = Query(3),
    claims: dict = Depends(validate_token),
    _: None = Depends(require_admin),
    service: DashboardService = Depends()
):
    """
    Get detected knowledge gaps

    Returns gaps sorted by:
    - Occurrence count (how many times asked)
    - Average confidence (lower = more urgent)
    - Negative feedback count
    """
    tenant_id = claims.get("tenant_id")

    return await service.get_knowledge_gaps(
        tenant_id, status, min_occurrences
    )

# Response example:
{
    "gaps": [
        {
            "id": "uuid",
            "pattern": "pricing tier upgrade billing",
            "occurrence_count": 12,
            "avg_confidence": 0.35,
            "negative_feedback_count": 5,
            "example_questions": [
                "How do I upgrade my pricing tier?",
                "What happens to billing when I upgrade?",
                "Can I change plans mid-cycle?"
            ],
            "first_detected_at": "2025-01-15T08:00:00Z",
            "last_occurrence_at": "2025-01-17T14:30:00Z",
            "status": "detected",
            "suggested_action": "Create document explaining upgrade process and prorated billing"
        }
    ],
    "total_gaps": 5
}

@router.post("/gaps/{gap_id}/acknowledge")
async def acknowledge_gap(
    gap_id: str,
    resolution_notes: str,
    claims: dict = Depends(validate_token),
    _: None = Depends(require_admin)
):
    """Mark gap as acknowledged (admin is working on it)"""
    # Update gap status to 'acknowledged'
    pass

@router.post("/gaps/{gap_id}/resolve")
async def resolve_gap(
    gap_id: str,
    resolution_notes: str,
    claims: dict = Depends(validate_token),
    _: None = Depends(require_admin)
):
    """Mark gap as resolved (document added, issue fixed)"""
    # Update gap status to 'resolved'
    pass
```

### 4.4 Feedback Reports

```python
@router.get("/feedback/summary")
async def get_feedback_summary(
    days: int = Query(30, ge=1, le=365),
    claims: dict = Depends(validate_token),
    _: None = Depends(require_admin),
    service: DashboardService = Depends()
):
    """
    Get feedback summary with trends
    """
    tenant_id = claims.get("tenant_id")

    return await service.get_feedback_summary(tenant_id, days)

# Response example:
{
    "period_days": 30,
    "total_feedback": 458,
    "helpful": 378,
    "not_helpful": 80,
    "helpful_percentage": 0.825,
    "trend": {
        "previous_period_helpful_percentage": 0.79,
        "change": +0.035,  # +3.5% improvement
        "direction": "improving"
    },
    "feedback_by_week": [
        {"week": "2025-W01", "helpful": 45, "not_helpful": 8},
        {"week": "2025-W02", "helpful": 52, "not_helpful": 12},
        {"week": "2025-W03", "helpful": 48, "not_helpful": 10}
    ],
    "messages_with_comments": 23,
    "common_feedback_themes": [
        {"theme": "incomplete_answer", "count": 12},
        {"theme": "wrong_information", "count": 5},
        {"theme": "helpful_and_clear", "count": 45}
    ]
}
```

### 4.5 Export Reports

```python
@router.get("/export/csv")
async def export_quality_report(
    start_date: str,
    end_date: str,
    report_type: str = Query(..., regex="^(sessions|feedback|gaps)$"),
    claims: dict = Depends(validate_token),
    _: None = Depends(require_admin),
    service: DashboardService = Depends()
):
    """
    Export quality reports as CSV

    Report types:
    - sessions: All sessions with quality metrics
    - feedback: All feedback with comments
    - gaps: All knowledge gaps with examples
    """
    tenant_id = claims.get("tenant_id")

    csv_content = await service.export_report(
        tenant_id, start_date, end_date, report_type
    )

    return Response(
        content=csv_content,
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename=quality_report_{report_type}_{start_date}_{end_date}.csv"
        }
    )
```

---

## 5. Implementation Phases

### Phase 1: Foundation (Week 1, Days 1-2)

**Tasks**:
1. Create service directory structure
2. Set up virtual environment and dependencies
3. Configure database connection (PostgreSQL)
4. Create Alembic migrations for all tables
5. Set up structured logging (Loguru + Structlog)
6. Implement JWT authentication (validate_token dependency)
7. Create health check endpoint

**Deliverables**:
- Service runs on port 8005
- Database tables created
- Authentication works
- Health check returns 200 OK

**Estimated Time**: 16 hours (2 days)

---

### Phase 2: Feedback Collection (Week 1, Days 3-5)

**Tasks**:
1. Implement AnswerFeedback model and schema
2. Create feedback submission API endpoint
3. Implement FeedbackService with validation
4. Set up RabbitMQ event publishing for feedback
5. Create feedback retrieval endpoints
6. Write unit tests for feedback service

**Deliverables**:
- Users can submit üëç/üëé feedback via API
- Feedback stored in database with tenant isolation
- Feedback events published to RabbitMQ
- Tests pass with >80% coverage

**Estimated Time**: 24 hours (3 days)

---

### Phase 3: RAG Quality Metrics (Week 2, Days 1-3)

**Tasks**:
1. Modify chat-service to include quality metrics in events
2. Implement RAGQualityMetrics model
3. Create QualityAnalyzer service
4. Set up RabbitMQ consumer for chat.message.created events
5. Implement quality metric storage and indexing
6. Add optional basic sentiment analysis (VADER)
7. Create quality metric retrieval endpoints
8. Write integration tests

**Deliverables**:
- Chat-service publishes enhanced events with RAG metrics
- Answer-quality-service consumes and stores metrics
- Quality data available via API
- Low-confidence answers flagged automatically
- Optional sentiment tracked

**Estimated Time**: 24 hours (3 days)

---

### Phase 4: Knowledge Gap Detection (Week 2, Days 4-5 + Week 3, Day 1)

**Tasks**:
1. Implement KnowledgeGap model
2. Create GapDetector service with clustering algorithm
3. Implement TF-IDF based question similarity
4. Create scheduled job for gap detection (daily)
5. Implement gap CRUD operations
6. Add gap acknowledgment/resolution workflow
7. Write tests for gap detection algorithm

**Deliverables**:
- Automatic detection of recurring low-confidence questions
- Knowledge gaps clustered by similarity
- Admin can acknowledge and resolve gaps
- Gap detection runs daily via scheduler
- Gap events published to RabbitMQ

**Estimated Time**: 20 hours (2.5 days)

---

### Phase 5: Session Quality Tracking (Week 3, Days 2-3)

**Tasks**:
1. Implement SessionQuality model
2. Create SessionTracker service
3. Implement session success calculation logic
4. Add session quality update triggers (on feedback, on message)
5. Create session quality aggregation queries
6. Write tests for session success algorithm

**Deliverables**:
- Session-level quality metrics calculated automatically
- Success indicators tracked (feedback, confidence, length)
- Session quality data available via API

**Estimated Time**: 16 hours (2 days)

---

### Phase 6: Admin Dashboard (Week 3, Days 4-5)

**Tasks**:
1. Implement DashboardService
2. Create dashboard overview endpoint
3. Create problematic sessions endpoint
4. Create knowledge gaps listing endpoint
5. Create feedback summary endpoint
6. Implement CSV export functionality
7. Add filtering and pagination
8. Write API integration tests

**Deliverables**:
- Complete admin dashboard API
- Overview metrics (success rate, feedback, gaps)
- Session drill-down with issue filtering
- Knowledge gap management
- Feedback trends and analysis
- CSV export for all reports

**Estimated Time**: 16 hours (2 days)

---

### Phase 7: Integration Testing (Week 4, Days 1-2)

**Tasks**:
1. Set up integration test environment
2. Write end-to-end tests for feedback flow
3. Write end-to-end tests for quality analysis
4. Write end-to-end tests for gap detection
5. Test RabbitMQ integration (producer/consumer)
6. Test multi-tenant isolation
7. Performance testing (load test with 1000 concurrent users)

**Deliverables**:
- All integration tests pass
- RabbitMQ integration verified
- Tenant isolation verified
- Performance benchmarks documented

**Estimated Time**: 16 hours (2 days)

---

### Phase 8: Documentation (Week 4, Day 3)

**Tasks**:
1. Write README.md with setup instructions
2. Document API endpoints (OpenAPI/Swagger)
3. Create integration guide for chat-service modifications
4. Document RabbitMQ message schemas
5. Create admin user guide for dashboard
6. Document quality metrics calculation logic
7. Create troubleshooting guide

**Deliverables**:
- Complete service documentation
- API documentation accessible at /docs
- Integration guide for developers
- Admin user guide for dashboard

**Estimated Time**: 8 hours (1 day)

---

### Phase 9: Deployment & Monitoring (Week 4, Days 4-5)

**Tasks**:
1. Create Dockerfile for answer-quality-service
2. Update docker-compose.yml with new service
3. Create database initialization scripts
4. Set up environment variables for production
5. Configure logging for production (JSON output)
6. Set up health check monitoring
7. Deploy to staging environment
8. Smoke testing in staging
9. Deploy to production

**Deliverables**:
- Service containerized and deployed
- Health monitoring configured
- Production environment validated
- Rollback plan documented

**Estimated Time**: 16 hours (2 days)

---

## 6. Success Metrics

### Development Metrics

- **Code Coverage**: >80% for all service modules
- **API Response Time**: <200ms for feedback submission, <500ms for dashboard queries
- **RabbitMQ Processing**: <100ms per message processing
- **Test Pass Rate**: 100% for unit and integration tests

### Business Metrics

- **Feedback Collection Rate**: Target >30% of AI responses get feedback
- **Answer Helpfulness**: Target >75% helpful feedback
- **Knowledge Gap Fill Rate**: Target >50% of detected gaps resolved within 30 days
- **Admin Adoption**: Target >80% of admins use dashboard weekly

### Quality Improvement Metrics

- **Answer Confidence Trend**: Track month-over-month improvement
- **Negative Feedback Trend**: Should decrease over time
- **Session Success Rate**: Should increase over time
- **Response Time**: Should remain stable or improve

---

## 7. Chat Service Integration

### Required Changes to Chat Service

The chat-service needs minor modifications to publish enhanced events:

**File**: `chat-service/app/services/chat_service.py`

```python
# ADD: Quality metrics extraction from RAG pipeline

class ChatService:
    async def generate_response(self, session_id: str, user_message: str, tenant_id: str):
        """Generate AI response with quality metrics"""

        start_time = time.time()

        # Existing RAG pipeline
        retrieval_results = await self.retrieve_documents(user_message, tenant_id)
        ai_response = await self.generate_with_llm(user_message, retrieval_results)

        response_time_ms = int((time.time() - start_time) * 1000)

        # NEW: Extract quality metrics
        quality_metrics = {
            "retrieval_score": self._calculate_retrieval_score(retrieval_results),
            "documents_retrieved": len(retrieval_results),
            "answer_confidence": self._extract_confidence(ai_response),
            "sources_cited": len(ai_response.get("sources", [])),
            "answer_length": len(ai_response["content"]),
            "response_time_ms": response_time_ms
        }

        # Store message (existing code)
        message = await self.store_message(session_id, "assistant", ai_response["content"])

        # NEW: Publish enhanced event with quality metrics
        await self.publish_message_created_event(
            tenant_id=tenant_id,
            session_id=session_id,
            message_id=message.id,
            message_type="assistant",
            content_preview=ai_response["content"][:200],
            quality_metrics=quality_metrics  # NEW
        )

        return ai_response

    def _calculate_retrieval_score(self, retrieval_results) -> float:
        """Calculate average relevance score of retrieved documents"""
        if not retrieval_results:
            return 0.0
        scores = [doc.get("score", 0.0) for doc in retrieval_results]
        return sum(scores) / len(scores)

    def _extract_confidence(self, ai_response) -> float:
        """Extract confidence score from LLM response metadata"""
        # Different LLMs expose confidence differently
        # For OpenAI: could use logprobs or response metadata
        # For now, use a heuristic based on response characteristics

        metadata = ai_response.get("metadata", {})

        # If LLM provides confidence, use it
        if "confidence" in metadata:
            return metadata["confidence"]

        # Otherwise, estimate based on:
        # - Presence of hedging language ("maybe", "possibly", "I'm not sure")
        # - Retrieval score (high retrieval = high confidence)
        # - Response length (very short might indicate uncertainty)

        content = ai_response["content"].lower()
        hedging_words = ["maybe", "possibly", "not sure", "might be", "could be", "uncertain"]
        hedging_count = sum(1 for word in hedging_words if word in content)

        retrieval_score = metadata.get("retrieval_score", 0.5)

        # Simple confidence calculation
        confidence = retrieval_score * (1 - (hedging_count * 0.1))
        return max(0.0, min(1.0, confidence))  # Clamp to [0, 1]
```

**Estimated Integration Time**: 4 hours

---

## 8. Comparison: Answer Quality vs Sentiment Analysis

### Development Time

| Feature | Answer Quality Service | Sentiment Analysis Service |
|---------|----------------------|---------------------------|
| Infrastructure setup | 2 days | 2 days |
| Feedback collection | 3 days | N/A |
| Quality metrics | 3 days | N/A |
| ML model setup | N/A | 5 days (training, deployment) |
| Sentiment analysis | Optional (VADER, 1 day) | 5 days (BERT/transformers) |
| Knowledge gap detection | 2.5 days | N/A |
| Session tracking | 2 days | 2 days |
| Admin dashboard | 2 days | 3 days |
| Testing & docs | 3 days | 4 days |
| **Total** | **2-3 weeks** | **3-4 weeks** |

### ROI for Knowledge-Base Chatbot

| Metric | Answer Quality | Sentiment Analysis |
|--------|---------------|-------------------|
| Actionable insights | ‚úÖ High (gaps, feedback) | ‚ö†Ô∏è Medium (limited for RAG) |
| User feedback clarity | ‚úÖ Direct (üëç/üëé) | ‚ö†Ô∏è Inferred (less accurate) |
| Guides improvement | ‚úÖ Specific (missing docs) | ‚ö†Ô∏è General (user mood) |
| Development cost | ‚úÖ Lower (2-3 weeks) | ‚ùå Higher (3-4 weeks) |
| Infrastructure needs | ‚úÖ Simple (no ML) | ‚ùå Complex (ML serving) |
| Maintenance burden | ‚úÖ Low (rule-based) | ‚ùå High (model updates) |
| False positive rate | ‚úÖ Low (user decides) | ‚ö†Ô∏è Medium (ML errors) |

### When Sentiment Analysis Would Be Valuable

Sentiment analysis makes sense for:
- **Customer support chatbots** (detect frustration, escalate to human)
- **Mental health chatbots** (detect distress, provide resources)
- **Sales chatbots** (detect buying intent, prioritize leads)
- **Feedback-free environments** (no thumbs up/down, need inference)

For RAG knowledge-base chatbots, **Answer Quality Service provides better ROI**.

---

## 9. Optional Enhancements (Future Phases)

### 9.1 Advanced Analytics

- **Trending topics**: What are users asking about most?
- **Time-series analysis**: Quality trends over weeks/months
- **Comparative analysis**: Quality by document source
- **User segmentation**: Power users vs new users

### 9.2 Automated Suggestions

- **Document recommendations**: "Based on gaps, suggest uploading docs about X"
- **Query refinement**: Suggest better ways to phrase questions
- **Auto-categorization**: Automatically tag sessions by topic

### 9.3 Integration with Onboarding Service

- **Document quality scores**: Track which documents produce best answers
- **Website scraping feedback**: Which scraped sites are most useful?
- **Content optimization**: Identify documents that need updating

### 9.4 Notifications

- **Slack/email alerts**: When critical gaps detected
- **Weekly digest**: Summary of quality metrics for admins
- **Tenant reports**: Monthly quality report sent to tenant admins

### 9.5 A/B Testing Support

- **Prompt variations**: Test different system prompts
- **Retrieval strategies**: Test different RAG configurations
- **Quality comparison**: Measure impact of changes

---

## 10. Risk Mitigation

### Technical Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|-----------|
| RabbitMQ downtime | Low | High | Implement retry logic, dead-letter queues |
| Chat-service integration breaks | Medium | High | Thorough integration tests, feature flags |
| Gap detection false positives | Medium | Medium | Tune similarity threshold, manual review |
| Database performance issues | Low | Medium | Proper indexing, query optimization |
| VADER sentiment inaccuracy | Medium | Low | Mark as optional, use only for frustration detection |

### Business Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|-----------|
| Low feedback collection rate | High | Medium | Make feedback UI prominent, incentivize feedback |
| Admins ignore dashboard | Medium | High | User training, actionable insights, notifications |
| Gaps not resolved | Medium | High | Prioritization system, integration with onboarding |
| Feature creep | Medium | Medium | Strict phase adherence, MVP first |

---

## 11. Deployment Checklist

### Pre-Deployment

- [ ] All tests passing (unit, integration, e2e)
- [ ] Code review completed
- [ ] Documentation updated
- [ ] Environment variables configured
- [ ] Database migrations tested
- [ ] RabbitMQ queues created
- [ ] Load testing completed
- [ ] Security review (SQL injection, XSS, auth)

### Deployment

- [ ] Database migrations run
- [ ] Service deployed to staging
- [ ] Smoke tests pass in staging
- [ ] Chat-service integration verified
- [ ] RabbitMQ consumer running
- [ ] Health checks green
- [ ] Logs streaming correctly
- [ ] Admin dashboard accessible
- [ ] Deploy to production
- [ ] Verify production health

### Post-Deployment

- [ ] Monitor error rates (first 24 hours)
- [ ] Verify message consumption rate
- [ ] Check database performance
- [ ] Validate feedback submissions
- [ ] Test admin dashboard access
- [ ] Collect initial feedback from admins
- [ ] Document any issues encountered
- [ ] Plan Phase 2 enhancements

---

## 12. Conclusion

The **Answer Quality & Feedback Service** provides a pragmatic, high-ROI alternative to full sentiment analysis for knowledge-base RAG chatbots. By focusing on:

1. **Direct user feedback** (üëç/üëé)
2. **RAG quality metrics** (retrieval, confidence)
3. **Knowledge gap detection** (missing content)
4. **Session success tracking** (resolution rates)
5. **Optional basic sentiment** (frustration detection only)

We deliver a service that:
- **Costs less** to build and maintain (2-3 weeks vs 3-4 weeks)
- **Provides clearer insights** (what's broken vs how users feel)
- **Guides action** (add this document vs user is sad)
- **Scales easily** (no ML infrastructure needed)
- **Integrates simply** (minimal chat-service changes)

### Next Steps

1. ‚úÖ Review this plan with project owner
2. ‚è≥ Approve Phase 1 to begin implementation
3. ‚è≥ Schedule kickoff meeting for development team
4. ‚è≥ Set up project tracking (Jira, Trello, etc.)
5. ‚è≥ Begin Phase 1: Foundation (Week 1, Days 1-2)

**Estimated Total Time**: 2-3 weeks (112-136 hours)
**Risk Level**: Low (well-defined requirements, proven patterns)
**Business Value**: High (directly improves answer quality, user satisfaction)